---
layout: post
title: Orders of Time Complexity, Big and Little 'O'
---

###Time Complexity and the Big 'O'

The previous post discussed that one factor in selecting a data structures is the efficiency of a data structure in running the most common operations. To understand the efficiencies of data structures, a working knowledge of time complexity is required. The time complexity of an algorithm measures the increase in the runtime of executing a particular operation in relation to the size of the inputs, as the inputs grow towards infinity. This is often described in the language of "Big O notation," which I'll briefly go over. Big O notation is a way of describing this time complexity, but actually describes the nature of the runtime in the ***worst case scenaro***. I'll cover some nuances about it at the end. A non-exhaustive list of orders of time complexity, ranging from preferable to least preferable:

* **Constant Time** - The fastest performances are at constant time, meaning the runtime of the operation is the same regardless of how large or long the input is. An example would be getting the value of the first index of an array (it's constant since you only need to access one cell despite how large an array may be). Another would be adding a value to the top of the stack, which takes the same amount of time no matter the size of the stack - we're simply tossing it on top. The notation for constant time is O(1), meaning that as time grows, the function still takes the same 1 unit of time.

* **Linear Time** - Linear time describes when the time to run the operation increases linearly as the size and/or length of the inputs increase (be it half as fast as the length of the input, or double as fast, etc.). An example would be searching for value in a linked list. The more values in a linked list, the more links that would have to be searched through to find the target, and the longer the runtime. The big O notation is O(n), where as inputs increase, the runtime is some n * the input size.

* **Logarithmic Time** - Logarithmic time describes when the time to run the operation increases linearly as the rate of log(n) as the inputs' size, n, grows towards infinity. This can be simplified to the following: every additional increase in the inputs' size is executed at a quicker rate than that of all the previous executions. An example would be searching for values in a binary search tree. While going from 1 node in a sorted binary search tree - which has a minimum depth of one, to 2 nodes in a binary search tree - which has a minimum depth of 2, we double the minimum depth and thus the time to search through the tree, despite only adding a single extra node. If there were more inputs though, like adding an extra value to a binary tree with 16 inputs already, the 17th input may not even add any additional time to the tree search. The minimum depth of a 16-node tree and a 17-node tree are both 5, so the search could have the same runtime despite the increase in a node. The big O notation is O(log(n)), increasing run time at log(n) where n is inputs' size.

* **Quadratic Time** - Quadratic time describes when the time to run the operation increases at a factor of n^2 where n is the inputs' size, growing towards infinity. It is a subset of **polynomial time**, when the runtime increases at some exponential value relative to the inputs, be it n^2, or n^3, etc. An example of quadratic time would be a function that creates a n-by-n sized grid for number values in a data set. As the size of the inputs grow, the runtime of the operation grows exponentially. When n=1, the 1x1 grid has a single cell to create, whereas if n=2, the 2x2 grid would have 4 cells to create, and so on, creating exponentially more work and longer runtime. The big O notation is O(n^2), increasing run time at n^2 where n is inputs' size as it approaches infinity.

###Little 'o' notation and the sub- prefix###

Other helpful terms are **sublinear** and **subquadratic**, which are directly related to their similar namesake. These two ideas can be destinguished from their similar namesake, by their "little 'o' notation," or lack thereof. Big O notation describes the runtime of the operation given the ***worst case*** input. For example, the Big O complexity of searching through a stack is linear. In the worst case, we have to search through the very bottom of the stack, and that increases directly in difficulty with the size of the stack. It doesn't matter that what we are searching for may actually be on top of the stack, our big O notation is based on the runtime of our worst case scenario. 

Small 'o' notation describes a function where the runtime in ALL CASES increases directly with the size of inputs. An example of this would be adding the sum of all values in an array of numbers. As the number of inputs into the array increases, the amount of steps taken to find the sum always increases in direct proportion. There is no case where increasing adding an additional input won't add additional runtime. Thus, the time complexity of finding the sum of an array not only can be expressed as O(n), but it is fits the small o notation: ***o(n)***. The previous example of searching for a value in a stack is O(n) but *not* o(n). It's O(n) because of what happens when our target search value is at the bottom of the stack, but it is *not* o(n) because of what happens if our target is on the top of the stack already. 

Sublinear describes a function that can be expressed as O(n), but not o(n). The previous example of the stack search has sublinear time complexity. Similarly, a subquadratic time complexity describes a function that can be expressed as O(n^2), but not o(n^2). An example of a subquadratic function would be searching through a nested array table. If the value is in the last column and last row, it'll search through the whole nested array - thus O(n^2). But if the value is in the first row and first column, it'll be an extremely fast runtime; thus it is not o(n^2), or syntactically it is ~o(n^2). 

Though the pairs of sublinear and linear, and subquadratic and quadratic, may have the same big O notations as each other, there are significant performance increases in having a function that is ~o(n) over one that is o(n), and ~o(n^2) function over one that's o(n^2). Thus, sublinear time complexity is a favorable subset of the linear time complexity order, and this applies analogously for subquadratic and quadratic.